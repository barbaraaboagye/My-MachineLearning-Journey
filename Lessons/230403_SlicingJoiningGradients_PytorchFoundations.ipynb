{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "886467ac-de7e-46bc-a55e-6fd21277147b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1> My Machine Learning Journey</h1>\n",
    "Civil Engineer · Researcher Youtuber · Machine Learning Engineer (The Goal)\n",
    "    <br>\n",
    "    Sharing and documenting my progress and journey as I learn machine learning\n",
    "     <br>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <a target=\"_blank\" href=\"https://github.com/barbaraaboagye/My-MachineLearning-Journey\"><img src=\"https://img.shields.io/github/last-commit/barbaraaboagye/My-MachineLearning-Journey\"></a>&nbsp;\n",
    "      <a target=\"_blank\" href=\"https://www.youtube.com/channel/UCEYKFq7ZEg81GYxpzNqYZ4)\"><img src=\"https://img.shields.io/youtube/channel/subscribers/UCEYKFq7ZEg81GYxpzNqYZ4Q?style=social\"></a>&nbsp;\n",
    "    <a target=\"_blank\" href=\"https://fr.linkedin.com/in/barbara-aboagye-233ba8133\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
    "    <a target=\"_blank\" href=\"https://twitter.com/awesome_ama\"><img src=\"https://img.shields.io/twitter/follow/awesome_ama?style=social\"></a>\n",
    "    <br>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45b714aa-02f9-45ac-b2bf-e83b34b0c093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Slicing, Joining and Gradients - Day x of x\n",
    "\n",
    "Hours completed : 1hr 30min <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39e4c6e7-9871-4211-8c4e-94bc180dc5f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Resources used : \n",
    "\n",
    "- [Foundation- PyTorch](https://madewithml.com/courses/foundations/pytorch/)\n",
    "<br>\n",
    "\n",
    "Handwritten notes can be found [here][def]\n",
    "\n",
    "[def]: https://github.com/barbaraaboagye/My-MachineLearning-Journey/blob/a4c272eca2f897b2d0cc32afab1afa5f58d739f2/Handwritten%20notes/230403_SlicingJoiningGradients_PytorchFoundations.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "409dd568",
   "metadata": {},
   "source": [
    "### Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d27bb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33186f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "684edb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fab70f43750>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "\n",
    "np.random.seed(seed=SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0866f3",
   "metadata": {},
   "source": [
    "### Basics \n",
    "\n",
    "We'll first cover some basics with PyTorch such as creating tensors and converting from common data structures (lists, arrays, etc.) to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e67177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 0.0461,  0.4024, -1.0115],\n",
      "        [ 0.2167, -0.6123,  0.5036]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a random tensor\n",
    "\n",
    "x = torch.randn(2,3) # normal distribution (rand(2,3) -> uniform distribution)\n",
    "print (f\"Type: {x.type()}\")\n",
    "print (f\"Size: {x.shape}\")\n",
    "print (f\"Values: \\n{x}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32c688ed",
   "metadata": {},
   "source": [
    "This code creates a tensor `x` of size `(2,3)` filled with random values from a normal distribution with mean 0 and standard deviation 1 using PyTorch.\n",
    "\n",
    "The tensor `x` has 2 rows and 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5fcc362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#Zeros and Ones tensor\n",
    "\n",
    "x = torch.zeros(2, 3)\n",
    "print(x)\n",
    "x = torch.ones(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac951d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3], [4, 5, 6])\n",
      "(2, 3)\n",
      "Type: 2\n",
      "Size: torch.Size([2, 3])\n",
      "x:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "Size: torch.Size([2, 3])\n",
      "x:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# List to Tensor\n",
    "\n",
    "x = [1,2,3], [4,5,6 ] # normal list\n",
    "print (x)\n",
    "print(np.shape(x))\n",
    "print(f\"Type: {np.ndim(x)}\")\n",
    "y = torch.Tensor([[1,2,3], [4,5,6 ]])\n",
    "print (f\"Size: {y.shape}\")\n",
    "print(f\"x:\\n {y}\")\n",
    "z = torch.Tensor(x)\n",
    "print (f\"Size: {z.shape}\")\n",
    "print(f\"x:\\n {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bad90e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([2, 3])\n",
      "x:\n",
      " tensor([[0.1915, 0.6221, 0.4377],\n",
      "        [0.7854, 0.7800, 0.2726]])\n"
     ]
    }
   ],
   "source": [
    "## NumPy array to Tensor\n",
    "\n",
    "x = torch.Tensor(np.random.rand(2,3))\n",
    "print (f\"Size: {x.shape}\")\n",
    "print(f\"x:\\n {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f9f36da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "tensor([[3.4332e-06, 1.0527e-11, 6.6651e-10, 2.1346e-07],\n",
      "        [5.4654e+22, 6.4591e-07, 1.3120e-08, 5.2713e-08],\n",
      "        [4.1652e-11, 7.1450e+31, 4.1418e-41, 0.0000e+00]])\n",
      "Type: torch.LongTensor\n",
      "tensor([[                   0,                    0,                    0,\n",
      "                            0],\n",
      "        [-9223372036854775808,                    0,                    0,\n",
      "                            0],\n",
      "        [                   0, -9223372036854775808,                    0,\n",
      "                            0]])\n"
     ]
    }
   ],
   "source": [
    "# Changing tensor type \n",
    "x = torch.Tensor(3,4)\n",
    "print(f\"Type: {x.type()}\")\n",
    "print(x)\n",
    "x = x.long()\n",
    "print(f\"Type: {x.type()}\")\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51f767dd",
   "metadata": {},
   "source": [
    "### Long tensor \n",
    "\n",
    "In programming, we often work with numbers, like 1, 2, 3, etc. However, sometimes we need to work with really large numbers that are too big to be represented using the normal integer data type.\n",
    "\n",
    "For example, imagine you have a dataset with billions of entries, and you need to index into it using unique identifiers that are assigned to each entry. In this case, you would need a data type that can represent really large integer values, and that's where LongTensor comes in.\n",
    "\n",
    "In PyTorch, a LongTensor is a special type of data structure that can hold large integer values ranging from -9223372036854775808 to 9223372036854775807. You can think of it like a container that can store really big numbers.\n",
    "\n",
    "You can create a LongTensor by either explicitly converting an existing tensor of a different data type using the .long() method or by directly initializing it using the torch.LongTensor() constructor. Once you have a LongTensor, you can use it for various mathematical operations, just like any other tensor in PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3d46265",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fc722ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 0.0761, -0.6775, -0.3988],\n",
      "        [ 3.0633, -0.1589,  0.3514]])\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "y = torch.randn(2,3)\n",
    "z = x + y\n",
    "print(f\"Size: {z.shape}\")\n",
    "print(f\"Values: \\n{z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "239f3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size : torch.Size([2, 2])\n",
      "Values : \n",
      " tensor([[ 1.0796, -0.0759],\n",
      "        [ 1.2746, -0.5134]])\n"
     ]
    }
   ],
   "source": [
    "## Dot product\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "y = torch.randn(3,2)\n",
    "z = torch.mm(x,y)\n",
    "print(f\"Size : {z.shape}\")\n",
    "print(f\"Values : \\n {z}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe74b735",
   "metadata": {},
   "source": [
    "The `torch.mm()` function is used to perform matrix multiplication between two tensors of size `(m,n)` and `(n,p)`, resulting in a tensor of size `(m,p)`. In other words, it multiplies each element of each row of the first tensor with each element of each column of the second tensor, and then sums the products to get the corresponding element of the resulting tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ad6233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size : torch.Size([2, 3])\n",
      "Values : \n",
      "tensor([[ 0.8042, -0.1383,  0.3196],\n",
      "        [-1.0187, -1.3147,  2.5228]])\n",
      "Size: torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[ 0.8042, -1.0187],\n",
      "        [-0.1383, -1.3147],\n",
      "        [ 0.3196,  2.5228]])\n"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "print(f\"Size : {x.shape}\")\n",
    "print (f\"Values : \\n{x}\")\n",
    "y = torch.t(x)\n",
    "print(f\"Size: {y.shape}\")\n",
    "print(f\"Values: \\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8881c953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values x:\n",
      " tensor([[ 0.4501,  0.2709, -0.8087],\n",
      "        [-0.0217, -1.0413,  0.0702]])\n",
      "Size: torch.Size([3, 2])\n",
      "Values z:\n",
      "tensor([[ 0.4501,  0.2709],\n",
      "        [-0.8087, -0.0217],\n",
      "        [-1.0413,  0.0702]])\n"
     ]
    }
   ],
   "source": [
    "## Reshape \n",
    "x = torch.randn(2,3)\n",
    "print(f\"Values x:\\n {x}\")\n",
    "z = x.view(3,2)\n",
    "print(f\"Size: {z.shape}\")\n",
    "print (f\"Values z:\\n{z}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e544d6b",
   "metadata": {},
   "source": [
    "This code reshapes the tensor `x` using the `.view()` method in PyTorch and assigns the resulting tensor to `z`.\n",
    "\n",
    "The `.view()` method in PyTorch is used to reshape a tensor to a new shape while maintaining the total number of elements in the tensor. In this case, the tensor `x` has dimensions `(2,3)` and a total of 6 elements.\n",
    "\n",
    "The `.view(3, 2)` method is used to reshape `x` into a tensor with dimensions `(3,2)` which also has a total of 6 elements. The resulting tensor `z` will have 3 rows and 2 columns.\n",
    "\n",
    "Note that when reshaping a tensor, the order of the elements in memory is preserved. In other words, the elements are simply rearranged to form the new shape without any change in their values. If the new shape does not have the same number of elements as the original tensor, a runtime error will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f205057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([2, 3, 4])\n",
      "x: \n",
      "tensor([[[ 1,  1,  1,  1],\n",
      "         [ 2,  2,  2,  2],\n",
      "         [ 3,  3,  3,  3]],\n",
      "\n",
      "        [[10, 10, 10, 10],\n",
      "         [20, 20, 20, 20],\n",
      "         [30, 30, 30, 30]]])\n",
      "\n",
      "\n",
      "Size: torch.Size([3, 8])\n",
      "a: \n",
      "tensor([[ 1,  1,  1,  1,  2,  2,  2,  2],\n",
      "        [ 3,  3,  3,  3, 10, 10, 10, 10],\n",
      "        [20, 20, 20, 20, 30, 30, 30, 30]])\n",
      "\n",
      "\n",
      "Size: torch.Size([3, 2, 4])\n",
      "b: \n",
      "tensor([[[ 1,  1,  1,  1],\n",
      "         [10, 10, 10, 10]],\n",
      "\n",
      "        [[ 2,  2,  2,  2],\n",
      "         [20, 20, 20, 20]],\n",
      "\n",
      "        [[ 3,  3,  3,  3],\n",
      "         [30, 30, 30, 30]]])\n",
      "\n",
      "\n",
      "Size: torch.Size([3, 8])\n",
      "c: \n",
      "tensor([[ 1,  1,  1,  1, 10, 10, 10, 10],\n",
      "        [ 2,  2,  2,  2, 20, 20, 20, 20],\n",
      "        [ 3,  3,  3,  3, 30, 30, 30, 30]])\n"
     ]
    }
   ],
   "source": [
    "# Dangers of reshaping (unintended consequences)\n",
    "x = torch.tensor([\n",
    "    [[1,1,1,1], [2,2,2,2], [3,3,3,3]],\n",
    "    [[10,10,10,10], [20,20,20,20], [30,30,30,30]]\n",
    "])\n",
    "print(f\"Size: {x.shape}\")\n",
    "print(f\"x: \\n{x}\\n\")\n",
    "\n",
    "a = x.view(x.size(1), -1)\n",
    "print(f\"\\nSize: {a.shape}\")\n",
    "print(f\"a: \\n{a}\\n\")\n",
    "\n",
    "b = x.transpose(0,1).contiguous()\n",
    "print(f\"\\nSize: {b.shape}\")\n",
    "print(f\"b: \\n{b}\\n\")\n",
    "\n",
    "c = b.view(b.size(0), -1)\n",
    "print(f\"\\nSize: {c.shape}\")\n",
    "print(f\"c: \\n{c}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6256753d",
   "metadata": {},
   "source": [
    "When we reshape a tensor, we need to be careful. Reshaping can change the order of the elements in the tensor, which can cause problems if we're not expecting it. In the code above, we first create a three-dimensional tensor `x`. Then, we reshape it into a tensor a with dimensions `(3, 8)`. This flattens the tensor, so that all the elements are in a single row.\n",
    "\n",
    "Next, we transpose the first two dimensions of `x` to get a tensor `b`. However, `b` is not contiguous, which means that its elements are not in a simple, ordered sequence in memory. This can cause issues later on.\n",
    "\n",
    "Finally, we reshape `b` into a tensor c with dimensions `(2, 12)` using the .view() method. This could also cause issues, since the elements of b are not contiguous.\n",
    "\n",
    "So, when we reshape tensors, we need to be careful and make sure we understand the order of the element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6bf65ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: \n",
      "tensor([[ 0.5797, -0.0599,  0.1816],\n",
      "        [-0.6797, -0.2567, -1.8189]])\n",
      "Values: \n",
      "tensor([-0.1000, -0.3166, -1.6373])\n",
      "Values: \n",
      "tensor([ 0.7013, -2.7553])\n"
     ]
    }
   ],
   "source": [
    "## Dimensional operations\n",
    "x = torch.randn(2, 3)\n",
    "print(f\"Values: \\n{x}\")\n",
    "y = torch.sum(x, dim=0) # add each row's value for every column\n",
    "print(f\"Values: \\n{y}\")\n",
    "z = torch.sum(x, dim=1) # add each columns's value for every row\n",
    "print(f\"Values: \\n{z}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b475f",
   "metadata": {},
   "source": [
    "### Indexing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56e7820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tensor([[ 0.2111,  0.3372,  0.6638,  1.0397],\n",
      "        [ 1.8434,  0.6588, -0.2349, -0.0306],\n",
      "        [ 1.7462, -0.0722, -1.6794, -1.7010]])\n",
      "x[:1] : \n",
      "tensor([[0.2111, 0.3372, 0.6638, 1.0397]]) \n",
      "x[:1,1:3] \n",
      " tensor([[0.3372, 0.6638]]) \n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "print (f\"x: \\n{x}\")\n",
    "print (f\"x[:1] : \\n{x[:1]} \") # select first row\n",
    "print (f\"x[:1,1:3] \\n {x[:1,1:3]} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de7d1100",
   "metadata": {},
   "source": [
    "The second print statement displays a slice of the tensor `x` using indexing. Specifically, it uses the slice notation `x[:1]` to select the first row of the tensor.\n",
    "\n",
    "The syntax `x[:1, 1:3]` means \"select the first row (`:1`) and columns 1 to 2 (`1:3`) of the tensor `x`\". The resulting sub-tensor will have shape `(1, 2)`, since it has one row and two columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc0a78af",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "Slicing is a way to extract a portion of a sequence or an array in a specific order. \n",
    "\n",
    "In PyTorch, slicing is performed using the square bracket notation (`[]`). You can use slicing to extract a subset of elements from a tensor by specifying the starting and ending indices of the slice for each dimension. For example, `x[:, 2:4]` selects all rows and columns 2 to 3 of tensor `x`.\n",
    "\n",
    "Slicing can also be used to modify a tensor by assigning a new value to a slice of the tensor. For example, `x[1,:] = torch.zeros(3)` sets the second row of tensor `x` to all zeros.\n",
    "\n",
    "Slicing is a powerful technique that allows you to efficiently extract and manipulate subsets of data in tensors. It is commonly used in data processing, machine learning, and deep learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ef6dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values x: \n",
      "tensor([[ 0.6486,  1.7653,  1.0812],\n",
      "        [ 1.2436,  0.8971, -0.0784]])\n",
      "Values : \n",
      "tensor([[ 0.6486,  1.0812],\n",
      "        [ 1.2436, -0.0784]])\n",
      " Values : \n",
      "tensor([ 0.6486, -0.0784])\n"
     ]
    }
   ],
   "source": [
    "# Select with dimensional indices\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "print(f\"Values x: \\n{x}\")\n",
    "\n",
    "col_indices = torch.LongTensor([0,2])\n",
    "chosen = torch.index_select(x,dim = 1,index = col_indices) #values from column\n",
    "print(f\"Values : \\n{chosen}\")\n",
    "\n",
    "row_indices = torch.LongTensor([0,1])\n",
    "col_indices = torch.LongTensor([0,2])\n",
    "chosen = x[row_indices,col_indices]\n",
    "print(f\" Values : \\n{ chosen}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a509259a",
   "metadata": {},
   "source": [
    "In this code, a 2x3 tensor `x` is created using the `torch.randn()` method, which generates random numbers from a normal distribution with mean 0 and standard deviation 1. Then, two examples of selecting values from `x` using dimensional indices are demonstrated.\n",
    "\n",
    "The first example selects values from specific columns of the tensor `x`. A tensor `col_indices` is created that contains the indices of the columns to select (in this case, columns 0 and 2). The `torch.index_select()` method is used to select the specified columns of `x`. The `dim` parameter specifies the dimension along which to select values (in this case, dimension 1, corresponding to columns). The resulting tensor `chosen` will have shape `(2, 2)` since it has two rows and two columns.\n",
    "\n",
    "The second example selects values from specific rows and columns of the tensor `x`. Two tensors, `row_indices` and `col_indices`, are created that contain the indices of the rows and columns to select (in this case, row 0 column 0, and row 1 column 2). The indices are used to index into `x` directly using square brackets. The resulting tensor `chosen` will have shape `(2,)` since it has two elements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5efd18",
   "metadata": {},
   "source": [
    "### Why are we indexing with `LongTensor`?\n",
    "\n",
    "In PyTorch, tensors can have different data types such as `FloatTensor`, `DoubleTensor`, `LongTensor`, `IntTensor`, and so on. The data type of a tensor determines the precision of the values that it can hold and the operations that can be performed on it.\n",
    "\n",
    "In the example you provided, `col_indices` is a tensor that stores the column indices that we want to select from `x`. We use `torch.LongTensor` to create `col_indices` because it is an integer tensor.\n",
    "\n",
    "By default, PyTorch creates `FloatTensor` when you pass a list of floating-point values to the constructor, and it creates `LongTensor` when you pass a list of integers. If we had used `torch.Tensor([0, 2])` instead of `torch.LongTensor([0, 2])`, PyTorch would have created a `FloatTensor` with elements `0.0` and `2.0`, which would not be valid indices for `x`.\n",
    "\n",
    "Therefore, it is important to use the correct data type for the indices when selecting elements from a tensor. In this case, we use `torch.LongTensor` to ensure that the indices are valid and that the tensor indexing operation works as expected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72c40dfa",
   "metadata": {},
   "source": [
    "### Will using `IntTensor` work?\n",
    "\n",
    "Yes, using `IntTensor` will also work in most cases where `LongTensor` is used.\n",
    "\n",
    "In PyTorch, `IntTensor` and `LongTensor` are both integer data types that can be used to store and manipulate integer values. The main difference between them is their size. `IntTensor` is a 32-bit integer data type, while `LongTensor` is a 64-bit integer data type.\n",
    "\n",
    "For most applications, using `IntTensor` instead of `LongTensor` will work fine. However, if you are working with very large tensors or very large indices, you may need to use `LongTensor` to avoid overflow errors.\n",
    "\n",
    "In general, it is a good practice to use the correct data type for the specific use case. For example, if you are working with very large tensors or very large indices, you should use `LongTensor`. If you are working with smaller tensors or indices, you can use `IntTensor`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be6824f8",
   "metadata": {},
   "source": [
    "### Joining\n",
    "\n",
    "Tensors can be combined via **concatenation** or **stacking** operation, which are similar with [Numpy's joining operation](https://github.com/barbaraaboagye/My-MachineLearning-Journey/blob/052b4aa4d52db40e2ed5e4814943aa3849299ad2/Lessons/230323_TransposeReshapeJoinExpand_NumpyFoundation.ipynb) behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25a34f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5548, -0.0845,  0.5903],\n",
      "        [-1.0032, -1.7873,  0.0538]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "print (x)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d02a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5548, -0.0845,  0.5903],\n",
      "        [-1.0032, -1.7873,  0.0538],\n",
      "        [ 0.5548, -0.0845,  0.5903],\n",
      "        [-1.0032, -1.7873,  0.0538]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "## Concatenation\n",
    "\n",
    "y = torch.cat([x,x], dim = 0) #concat on a specified dimension\n",
    "print (y)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e48ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5548, -0.0845,  0.5903],\n",
      "         [-1.0032, -1.7873,  0.0538]],\n",
      "\n",
      "        [[ 0.5548, -0.0845,  0.5903],\n",
      "         [-1.0032, -1.7873,  0.0538]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Stacking \n",
    "\n",
    "z = torch.stack([x,x], dim = 0) # stack on new dimension\n",
    "print (z)\n",
    "print (z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc595ccd",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "We can determine gradients (rate of change) of our tensors with respect to their constituents using gradient bookkeeping. The gradient is a vector that points in the direction of greatest increase of a function. We'll be using gradients in the next lesson to determine how to change our weights to affect a particular objective function (ex. loss).\n",
    "\n",
    "Imagine you want to teach a computer how to recognize handwritten digits. To do this, you create a neural network that takes in images of digits as input, and produces a prediction of what digit is in the image as output. When you train the network, you show it many examples of handwritten digits along with the correct answer, and you adjust the weights of the network so that it produces the correct answer for each example.\n",
    "\n",
    "However, adjusting the weights manually would be very difficult because there are so many of them, and it would be very time-consuming. Instead, you use an optimization algorithm to automatically adjust the weights for you. The most common optimization algorithm used in deep learning is called stochastic gradient descent (SGD).\n",
    "\n",
    "SGD works by adjusting the weights in the direction of the steepest descent of the loss function. The loss function measures how well the network is doing at recognizing digits. During training, the network produces a prediction for each image, and the loss function compares the prediction to the correct answer. The goal of training is to minimize the loss function, which means that the network is doing a good job at recognizing digits.\n",
    "\n",
    "To adjust the weights using SGD, we need to compute the gradients of the loss function with respect to each weight in the network. These gradients tell us how much each weight needs to be adjusted to reduce the loss function. PyTorch makes it easy to compute gradients automatically using the `backward()` method. When you call `backward()` on the output of the network, PyTorch computes the gradients of the output with respect to all the weights in the network.\n",
    "\n",
    "Once we have the gradients, we can use them to adjust the weights using SGD. SGD multiplies the gradients by a small number called the learning rate, and subtracts the result from the weights. This adjusts the weights in the direction that reduces the loss function.\n",
    "\n",
    "Over time, as the weights are adjusted, the network gets better at recognizing digits. The gradients tell us how much to adjust the weights, and SGD does the adjusting automatically, so we don't have to do it manually.\n",
    "\n",
    "In summary, gradients are a key component of training neural networks. They tell us how much to adjust the weights of the network to produce a better output, and they are computed during the backpropagation proces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "553ddf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tensor([[0.7379, 0.0846, 0.4245, 0.9778],\n",
      "        [0.6800, 0.3151, 0.3911, 0.8943],\n",
      "        [0.6889, 0.8389, 0.1780, 0.6442]], requires_grad=True)\n",
      "x.grad: \n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "tensor([[4.2138, 2.2539, 3.2734, 4.9334],\n",
      "        [4.0399, 2.9453, 3.1733, 4.6829],\n",
      "        [4.0667, 4.5168, 2.5341, 3.9325]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Tensors with gradient bookkeeping\n",
    "\n",
    "x = torch.rand(3, 4, requires_grad=True)\n",
    "y = 3*x + 2\n",
    "z = y.mean() #calculate the mean of y\n",
    "z.backward() # z has to be scalar\n",
    "print(f\"x: \\n{x}\")\n",
    "print(f\"x.grad: \\n{x.grad}\")\n",
    "print (y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86db03b3",
   "metadata": {},
   "source": [
    "In PyTorch, we can define tensors with a special attribute called `requires_grad`. When we set this attribute to `True`, PyTorch keeps track of all the operations that are performed on that tensor. The operations are recorded in a computational graph, which is then used to compute gradients of a function with respect to that tensor using backpropagation.\n",
    "\n",
    "In the given code snippet, we define a random tensor `x` of size `(3, 4)` and set its `requires_grad` attribute to `True`. We then perform some operations on `x` and define a new tensor `y` which is a linear transformation of `x`. We then define a scalar tensor `z` which is the mean of `y`. Finally, we call the `backward()` method on z to compute the gradients of `z` with respect to `x`. Since `z` is a scalar, we don't need to specify any arguments to the `backward()` method.\n",
    "\n",
    "After calling `backward()`, the gradients of `z` with respect to `x` are computed and stored in the grad attribute of `x`. We print the value of `x` and `x.grad` to see the values of `x` and its gradients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab8eec77",
   "metadata": {},
   "source": [
    "### CUDA\n",
    "CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) developed by NVIDIA for use with its graphics processing units (GPUs)\n",
    "\n",
    "In PyTorch, CUDA can be used to accelerate training and inference of deep learning models by offloading computations to the GPU. PyTorch provides an interface to easily move tensors to and from the GPU, and to execute operations on the GPU using CUDA. This can significantly speed up the computation time for large neural networks and large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9aaa1e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Is CUDA available?\n",
    "print (torch.cuda.is_available())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eef577ec",
   "metadata": {},
   "source": [
    "If False (CUDA is not available), let's change that by following these steps: Go to Runtime > Change runtime type > Change Hardware accelerator to GPU > Click Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd4a94a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d84d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,3)\n",
    "print (x.is_cuda)\n",
    "x = torch.rand(2,3).to(device) # Tensor is stored on the GPU\n",
    "print (x.is_cuda)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80019ceb",
   "metadata": {},
   "source": [
    "I am going to continue to use my CPU :-). Going to start a project of recognising handwritting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "02ffa660b6f6d9be8cca427b74bc9cb87fea4b28c9e17012788bc534d238c364"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
